\documentclass{article}
\usepackage{ml1_homework_template}

% please submit the corresponding pdf by email to
% homework@class,brml.org, and write "homework sheet xx" in the 
% title.  No more, no less!  (Instead of xx, however,
% put the decimal number of the homework sheet.)

% Please update the following line, only change XX to the homework
% sheet number
\title{homework sheet 02}


\author{
\name{Andre Seitz}\\
\imat{03622870}\\
\email{andre.seitz@mytum.de}
\And
\name{Linda Leidig} \\
\imat{03608416}\\
\email{linda.leidig@tum.de}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

%\usepackage{xyling}
\usepackage{qtree}


\usepackage{courier}
\usepackage{listings}
\lstset{
         basicstyle=\footnotesize\ttfamily, 
         numberstyle=\tiny,          
         numbersep=5pt,             
         tabsize=2,                
         extendedchars=true,      
         breaklines=true,        
         showspaces=false,      
         showtabs=false,       
         xleftmargin=17pt,
         framexleftmargin=17pt,
         framexrightmargin=5pt,
         framexbottommargin=4pt,
         showstringspaces=false 
 }
 \lstloadlanguages{
         Python
 }


\begin{document}
\maketitle

\section{Assignment: Learning by doing}

\paragraph*{Problem 1 and 2}
$\;$ 

Python code required for the solution:

\lstinputlisting[language=python]{python/decision_tree/node.py}

\lstinputlisting[language=python]{python/decision_tree/main.py}

Results:

1. Decision tree:


%\Tree{		& \K{$x_1 \leq 4.1$} \B{dl}\B{dr}\\ 
%\K{[0,6,0]} &					& \K{$x_1 \leq 6.9$}\B{dl}\B{dr}\\
%&					\K{[2,0,4]} &					& \K{[3,0,0]}}


\Tree [.{$x_1\leq4.1$ \\ g=0.658}
	{$[0,6,0]$ \\ g=0} 
	[.{$x_1\leq6.9$ \\ g=0.494 }
		{$[2,0,4]$ \\ g=0.444} 
		{$[3,0,0]$ \\ g=0} ] ]
		
		
		
2. Classification:

[4.1, -0.1,  2.2] belongs to class [1] with a probability of 1.0.

[6.1,  0.4,  1.3] belongs to class [2] with a probability of $\frac{2}{3}$.

\paragraph*{Problem 3 and 4}
$\;$ 

Python code required for the solution:

\lstinputlisting[language=python]{python/knn/kNN.py}

\lstinputlisting[language=python]{python/knn/main.py}

Results:

1. KNN:

[4.1, -0.1,  2.2] belongs to class [0, 1, 2] with a probability of $\frac{1}{3}$. Therefore, pick randomly class 1.

[6.1,  0.4,  1.3] belongs to class [2] with a probability of $\frac{2}{3}$.

2. KNN Regression:

Regression for [4.1, -0.1,  2.2] results in 0.561.

Regression for [6.1,  0.4, 1.3] results in 1.396.

\paragraph*{Problem 5}
$\;$ 

The values of the second column are much smaller than the values of the first and third column. Scaling the data could compensate this problem affecting the euclidian distance (standardization). Another solution could be using another distance measurement.

This problem does not arise when training a decision tree since only one feature is considered for splitting the set at a time. Therefore, only the relative differences within each column are important.



\section{Assignment: Probabilistic kNN}
\paragraph*{Problem 6}
$\;$ 

To show:
\begin{eqnarray}
\frac{p(c=0|x^*)}{p(c=1|x^*)} \approx \frac{e^{-\frac{{\|x^*-x_0\|}^2}{2\sigma^2}}}{e^{-\frac{{\|x^*-x_1\|}^2}{2\sigma^2}}}
\end{eqnarray}

We know from the lecture:
\begin{eqnarray}
p(c = b|x^*) = \frac{p(x^*|c=b)p(c=b)}{\sum_{i \in classes}{p(x^*|c=i)p(c=i)}}
\end{eqnarray}
and
\begin{eqnarray}
p(x | c = b) &=& \frac{1}{N_b} \sum_{n \in class b}{\mathcal{N}(x | x_n, \sigma^2 I)}\\
&=& \frac{1}{N_b} \frac{1}{(2\pi \sigma^2)^{D/2}} \sum_{n \in class b}{e^{-\frac{\|x -x_n\|^2}{2 \sigma^2}}}
\end{eqnarray}

Here we have the two classes 0 and 1 with $N_0$ and $N_1$ elements, respectively.
It holds:
\begin{eqnarray}
p(c = 0) &=& \frac{N_0}{N_0 + N_1}\\
p(c = 0) &=& \frac{N_1}{N_0 + N_1}
\end{eqnarray}

\begin{eqnarray}
\Rightarrow p(c = 0 | x^*) &=& \frac{p(x^* | c = 0)p(c = 0)}{p(x^* | c = 0)p(c = 0) + p(x^* | c = 1)p(c = 1)}\\
&=& \frac{\frac{1}{N_0} \frac{1}{(2\pi \sigma^2)^{D/2}} \sum_{n \in class 0}{e^{-\frac{\|x -x_n\|^2}{2 \sigma^2}}}\frac{N_0}{N_0 + N_1}}{p(x^* | c = 0)\frac{N_0}{N_0 + N_1} + p(x^* | c = 1)\frac{N_1}{N_0 + N_1}}\\
\Rightarrow p(c = 1 | x^*) &=& \frac{p(x^* | c = 1)p(c = 1)}{p(x^* | c = 0)p(c = 0) + p(x^* | c = 1)p(c = 1)}\\
&=& \frac{\frac{1}{N_1} \frac{1}{(2\pi \sigma^2)^{D/2}} \sum_{n \in class 1}{e^{-\frac{\|x -x_n\|^2}{2 \sigma^2}}}\frac{N_1}{N_0 + N_1}}{p(x^* | c = 0)\frac{N_0}{N_0 + N_1} + p(x^* | c = 1)\frac{N_1}{N_0 + N_1}}
\end{eqnarray}

This leads to the following transformations:
\begin{eqnarray}
\frac{p(c=0|x^*)}{p(c=1|x^*)} &=& \frac{\frac{1}{N_0} \frac{1}{(2\pi \sigma^2)^{D/2}} \sum_{n \in class 0}{e^{-\frac{\|x -x_n\|^2}{2 \sigma^2}}}\frac{N_0}{N_0 + N_1}}{\frac{1}{N_1} \frac{1}{(2\pi \sigma^2)^{D/2}} \sum_{n \in class 1}{e^{-\frac{\|x -x_n\|^2}{2 \sigma^2}}}\frac{N_1}{N_0 + N_1}}\\
&=& \frac{\sum_{n \in class 0}{e^{-\frac{\|x -x_n\|^2}{2 \sigma^2}}}}{\sum_{n \in class 1}{e^{-\frac{\|x -x_n\|^2}{2 \sigma^2}}}}\\
&=& \frac{e^{-\frac{\|x -x_0\|^2}{2 \sigma^2}} + \sum_{n \in class 0 \backslash x_0}{e^{-\frac{\|x -x_n\|^2}{2 \sigma^2}}}}{e^{-\frac{\|x -x_1\|^2}{2 \sigma^2}} + \sum_{n \in class 1 \backslash x_1}{e^{-\frac{\|x -x_n\|^2}{2 \sigma^2}}}}
\end{eqnarray}

Since $\sigma ^2$ is very small the last part of the sums is very small in comparison to the first part and therefore can be neglected.
This results in the approximation that was meant to be shown:
\begin{eqnarray}
\frac{p(c=0|x^*)}{p(c=1|x^*)} \approx \frac{e^{-\frac{{\|x^*-x_0\|}^2}{2\sigma^2}}}{e^{-\frac{{\|x^*-x_1\|}^2}{2\sigma^2}}}
\end{eqnarray}


\paragraph*{Problem 7}
$\;$ 

Since $\sigma \rightarrow 0$, we know from problem 6 that
\begin{eqnarray}
\frac{p(c = 0|x^*)}{p(c = 1|x^*)} \approx \frac{e^{-\frac{{\|x^*-x_0\|}^2}{2\sigma^2}}}{e^{-\frac{{\|x^*-x_1\|}^2}{2\sigma^2}}} &=& e^{-\frac{{\|x^*-x_0\|}^2}{2\sigma^2}+\frac{{\|x^*-x_1\|}^2}{2\sigma^2}} \label{basis}\\
&=& e^{\frac{-{\|x^*-x_0\|}^2+{\|x^*-x_1\|}^2}{2\sigma^2}}
\end{eqnarray}

If 
\begin{eqnarray}
0 < \frac{p(c = 0|x^*)}{p(c = 1|x^*)} < 1 \label{class1}\\
\Leftrightarrow p(c = 0|x^*) < p(c = 1|x^*)
\end{eqnarray} 
and therefore, class 1 is chosen for $x^*$.

If 
\begin{eqnarray}
\frac{p(c = 0|x^*)}{p(c = 1|x^*)} > 1 \label{class0}\\
\Leftrightarrow p(c = 0|x^*) > p(c = 1|x^*)
\end{eqnarray} 
and therefore, class 0 is chosen for $x^*$.

For $e^x$: If $x < 0$, then $0 < e^x < 1$. If $x > 0$, then $e^x > 1$.

Let $x = \frac{-{\|x^*-x_0\|}^2+{\|x^*-x_1\|}^2}{2\sigma^2}$.

If $x^*$ is closer to $x_0$ than to $x_1$, ${\|x^*-x_0\|}^2 < {\|x^*-x_1\|}^2$. This can further be transformed:
\begin{eqnarray}
\Leftrightarrow {\|x^*-x_0\|}^2 - {\|x^*-x_1\|}^2 &<& 0\\
\Leftrightarrow \frac{-{\|x^*-x_0\|}^2+{\|x^*-x_1\|}^2}{2\sigma^2} &>& 0\\
\Leftrightarrow x &>& 0
\end{eqnarray}

For $x > 0$ we know that $e^x > 1$ and therefore $e^{\frac{-{\|x^*-x_0\|}^2+{\|x^*-x_1\|}^2}{2\sigma^2}} > 1$. Since Equation \ref{basis}, we know $\frac{p(c = 0|x^*)}{p(c = 1|x^*)} > 1$, which let us come to the conclusion that class 0 is chosen if $x^*$ is closer to $x_0$ than to $x_1$ (see Equation \ref{class0}).


\paragraph*{Problem 8}
$\;$ 

The higher the variance, the more neighbours should be considered since it is more probable that there is an element from another class in between the elements of the depicted class due to the high variance. The smaller the variance, the smaller the number of considered neighbours. Due to the small variance the probability of finding an element from another class in the area of the depicted class is small.

Additionally, the approximation of Problem 6 indicates this correlation. A very small variance is considered which allows the disregard of all neighbours except the nearest neighbour.

\section{Assignment: Neighbourhood Component Analysis}

\paragraph*{Problem 9}
$\;$ 

...


\end{document}
