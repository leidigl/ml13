\documentclass{article}
\usepackage{ml1_homework_template}
\usepackage{amsmath}
\usepackage{amssymb}

% please submit the corresponding pdf by email to
% homework@class,brml.org, and write "homework sheet xx" in the 
% title.  No more, no less!  (Instead of xx, however,
% put the decimal number of the homework sheet.)

% Please update the following line, only change XX to the homework
% sheet number
\title{homework sheet 05}


\author{
\name{Andre Seitz}\\
\imat{03622870}\\
\email{andre.seitz@mytum.de}
\And
\name{Linda Leidig} \\
\imat{03608416}\\
\email{linda.leidig@tum.de}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.


\renewcommand{\Vec}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\Mtx}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}


\begin{document}
\maketitle

\section{Assignment: Probability Theory}
\paragraph*{Problem 1}
$\;$ 

Let $X$ have a continuous cdf $F_X(x)$. Define the
random variable $Y$ as $Y = F_X(X)$. Assuming that
$F_X(x)$ is strictly increasing, how is $Y$ distributed?
Show your work.

\paragraph*{Solution:}
\begin{eqnarray}
P(Y \leq y) = P(F_X(X) \leq y) = P(X \leq F^{-1}_X(y)) = F_X(F_X^{-1}(y)) = y
\end{eqnarray}

The area under a curve (integral) between 0 and some point $y \in [0,1]$ can only be the same as $y$ if it is a constant curve at 1. Therefore, $Y$ is uniformly distributed with $U(0,1)$.

\paragraph*{Problem 2}
$\;$ 

Show that the sum of two independent Gaussian random variables ($\Vec{X_1}$ and $\Vec{X_2}$) is Gaussian. Some of the properties of Gaussians mentioned in the lecture can help.

\paragraph*{Solution:}
Given
\begin{eqnarray}
\Vec{X_1} = \mathcal{N}(x | \mu_1, \Sigma_1) \\
\Vec{X_2} = \mathcal{N}(x | \mu_2, \Sigma_2) \\
\end{eqnarray}
we look for 
\begin{eqnarray}
\Vec{Z} = \Vec{X_1} + \Vec{X_2} \overset{!}{=} \mathcal{N}(x | \mu_Z, \Sigma_Z)
\end{eqnarray}
From the lecture we know 
\begin{eqnarray}
\Vec{Y} = \Mtx{A}\Vec{X} + \Mtx{B}
\end{eqnarray}
with $\Mtx{X}$ being a Gaussian, $\Vec{Y}$ being one as well.\\
We chose $\Vec{X}$ as $(\Vec{X_1},\Vec{X_2})^T$. As known from the lectures $\Vec{X}$ is also a (multivariate) Gaussian with $\mu_Y = (\mu_1,\mu_2)^T$ and $\Sigma_Y = \left( \begin{array}{c c} \Sigma_1 & 0 \\ 0 & \Sigma_2 \end{array}\right)$.\\
$\Vec{Y}$ can therefore be written as 
\begin{eqnarray}
\Vec{Y} = \Mtx{A} (\Vec{X_1},\Vec{X_2})^T + \Mtx{B}
\end{eqnarray}
Choosing $\Mtx{A} = (1,1)$ and $\Mtx{B} = 0$ leads to
\begin{eqnarray}
\Vec{Y} = (1,1) (\Vec{X_1},\Vec{X_2})^T = \Vec{X_1} + \Vec{X_2} = \Vec{Z}
\end{eqnarray}
Therefore $\Vec{Z}$ has to be a Gaussian with mean and variance:
\begin{eqnarray}
\mu_Z = \mu_1+\mu_2 \\
\Sigma_Z = \Sigma_1 + \Sigma_2
\end{eqnarray}

\paragraph*{Problem 3}
$\;$ 

Let $Z = (X, Y)$ be a bivariate normal distributed random variable. Furthermore, let $X \sim \mathcal{N}(\mu_X, \sigma^2_X)$ and $Y \sim \mathcal{N}(\mu_Y, \sigma^2_Y)$. Assume that $\rho(X, Y) = 0$. Show that in this case $X$ and $Y$ are independent.

\paragraph*{Solution:}
Two random variables $X$ and $Y$ are independent if the following holds:
\begin{eqnarray}
f(x,y) = f(x)f(y)
\label{inequality}
\end{eqnarray}
with $f(x)$ and $f(y)$ being the density function of $X$ and $Y$, respectively. $f(x,y)$ is the joint density function.

Since $Z$ is a bivariate Gaussian composed of $X$ and $Y$, $z = \left( \begin{array}{c} x\\y\end{array} \right)$ and $\mu_Z = \left(\begin{array}{c}\mu_X\\\mu_Y\end{array}\right)$. The correlation coefficient of X and Y is zero. Therefore, the covariance of X and Y is zero, which leads to the following covariance matrix for $Z$:
\begin{eqnarray}
\Sigma_Z = \left( \begin{array}{cc}
\sigma_X^2 & 0\\
0 & \sigma_Y^2
\end{array} \right)
\end{eqnarray}

We can use this to show equation \ref{inequality}:

\begin{eqnarray}
f(z) &=& \frac{1}{\sqrt{|2\pi \Mtx{\Sigma_Z}}|} exp\left(-\frac{1}{2} ((\Vec{z}-\Vec{\mu_Z})^T \Mtx{\Sigma_Z} (\Vec{z}-\Vec{\mu_Z})\right)\\
f(x,y) &=& \frac{1}{\sqrt{\left|2\pi \left( \begin{array}{cc}
\sigma_X^2 & 0\\
0 & \sigma_Y^2
\end{array} \right) \right| }}
 exp\left(-\frac{1}{2} \left( \left( \begin{array}{c} x-\mu_X \\y-\mu_Y \end{array} \right)^T 
 \left( \begin{array}{cc}
\sigma_X^2 & 0\\
0 & \sigma_Y^2
\end{array} \right)
 \left( \begin{array}{c} x-\mu_X \\y-\mu_Y \end{array} \right) \right) \right)\\
 &=& \frac{1}{\sqrt{2^2\pi^2 \sigma_X^2 \sigma_Y^2}}
 exp\left(-\frac{1}{2} \left( \left( \begin{array}{c} \frac{x-\mu_X}{\sigma_X^2} \\ \frac{y-\mu_Y}{\sigma_Y^2} \end{array} \right)^T 
 \left( \begin{array}{c} x-\mu_X \\y-\mu_Y \end{array} \right) \right) \right)\\
 &=& \frac{1}{2\pi \sigma_X \sigma_Y}
 exp\left(-\frac{1}{2} \left( \frac{(x-\mu_X)^2}{\sigma_X^2} + \frac{(y-\mu_Y)^2}{\sigma_Y^2} \right) \right)\\
 &=& \frac{1}{2\pi \sigma_X \sigma_Y}
 exp\left(-\frac{1}{2} \frac{(x-\mu_X)^2}{\sigma_X^2}\right) \cdot exp\left(-\frac{1}{2} \frac{(y-\mu_Y)^2}{\sigma_Y^2} \right)\\
 &=& \frac{1}{\sqrt{2\pi} \sigma_X}
 exp\left(-\frac{1}{2} \frac{(x-\mu_X)^2}{\sigma_X^2}\right) \cdot \frac{1}{\sqrt{2\pi} \sigma_Y} exp\left(-\frac{1}{2} \frac{(y-\mu_Y)^2}{\sigma_Y^2} \right)\\
 &=& f(x)f(y)
\end{eqnarray}


\section{Assignment: Weighted Linear Regression}
\paragraph*{Problem 4}
$\;$ 

Consider a linear regression problem in which we want to ``weight'' 
different training examples differently. Specifically, suppose we want to minimize
\[ 
E(\Vec{w}) = \frac{1}{2}\sum^N \theta_n\left(z_n- \Vec{w}^T\Vec{\phi}(\Vec{x}_n)\right)^2 
\]

We already worked out what happens for the case where all the
weights $\theta_n$ are the same. In this problem, we will generalize some 
of those ideas to the weighted setting, and also implement the locally 
weighted linear regression algorithm.

\begin{enumerate}
\item Show that $E(\Vec{w})$ can also be written
\begin{equation} \label{Emat}
E(\Vec{w}) = (\Vec{z}-\Mtx{\Phi}\Vec{w})^{T}\Mtx{\Theta}(\Vec{z}-\Mtx{\Phi}\Vec{w}) 
\end{equation}
for an appropriate diagonal matrix $\Mtx{\Theta}$, and where $\Mtx{\Phi}$ and 
$\Vec{z}$ are as defined in class. State clearly what $\Mtx{\Theta}$ is.

\paragraph*{Solution:}
We start with converting $E(w) = (z - \Phi w)^T \Theta (z - \Phi w)$ and will get $E(w) = \frac{1}{2} \sum_{n=1}^{N} \theta_n (z_n - w^T \phi(x_n))^2$:
\begin{eqnarray}
E(w) &=& (z-\Phi w)^T \Theta (z-\Phi w) \\
%
&=& \left[\left(  
\begin{array}{c} 
z_1 \\
\vdots \\
 z_n
\end{array}\right) - \left( 
\begin{array}{c} 
\phi_{1,1}w_1+\cdots +\phi_{M,1}w_M \\
\vdots \\
\phi_{1,N}w_1+\cdots + \phi_{M,N}w_M
\end{array}\right)\right]^T \Theta (z-\Phi w) \\
%
&=& \left[\left( 
\begin{array}{c} 
z_1 - (\phi_{1,1}w_1+\cdots +\phi_{M,1}w_M) \\
\vdots \\
z_N - (\phi_{1,N}w_1+\cdots + \phi_{M,N}w_M)
\end{array}\right)\right]^T \Theta (z-\Phi w) \\
%
&=& \left[\left( 
\begin{array}{c} 
\theta_{1,1}(z_1 - (\phi_{1,1}w_1+\cdots +\phi_{M,1}w_M)) \\
\vdots \\
\theta_{N,N}(z_N - (\phi_{1,N}w_1+\cdots + \phi_{M,N}w_M))
\end{array}\right)\right]^T (z-\Phi w) \\
%
&=& \theta_{1,1} (z_1 - (\phi_{1,1} w_1 + \cdots + \phi_{M,1} w_M))^2 + \cdots + \theta_{N,N} (z_n - (\phi_{1,N} w_1 + \cdots + \phi_{M,N} w_M))^2 \\
%
&=& \sum_{n=1}^N \theta_{n,n} (z_n - \sum_{m=1}^M \phi_{n,m} w_m)^2
\end{eqnarray}
Now we define $\theta_n$ as the diagonal entries of the diagonal matrix $\Theta$, where $\theta_{n,n} = \frac{1}{2} \theta_n$ and proceed transforming the matrix.
\begin{eqnarray}
\sum_{n=1}^N \theta_{n,n} (z_n - \sum_{m=1}^M \phi_n,m w_m)^2 &=&\\
%
\frac{1}{2} \sum_{n=1}^N \theta_{n} (z_n - \sum_{m=1}^M \phi_n,m w_m)^2 &=&\\
%
\frac{1}{2} \sum_{n=1}^N \theta_{n} (z_n - w^T \phi (x_n))^2 &=& E(w)
\end{eqnarray}
And just to ensure a clear statement: The matrix $\Mtx{\Theta} $ was chosen as $\Mtx{\Theta} = \left( 
\begin{array}{c c c c}  
\frac{1}{2}\theta_1 	& 0 					& \cdots 					& 0 \\
0 								& \ddots			& \ddots 					& \vdots \\
\vdots						& 	\ddots			& 	\ddots					&	0		\\
0								& \cdots			&	0							&\frac{1}{2}\theta_n
\end{array} \right)$  


\item Now let all the $\theta_n$ equal $1$. By differentiating Eq.\ \ref{Emat} 
with respect to \Vec{w}, derive the normal equations for the least squares problem, 
as given in class. 

\paragraph*{Solution:}
\begin{eqnarray}
\nabla_w E(\Vec{w}) &=& -\Mtx{\Phi}^T (\Mtx{\Theta} + \Mtx{\Theta}^T) (\Vec{z}-\Mtx{\Phi} \Vec{w}) = -\Mtx{\Phi}^T \Mtx{I} (\Vec{z}-\Mtx{\Phi} \Vec{w})\\
&=& -\Mtx{\Phi}^T (\Vec{z}-\Mtx{\Phi} \Vec{w}) = \Mtx{\Phi}^T \Mtx{\Phi} \Vec{w} - \Mtx{\Phi}^T \Vec{z}
\end{eqnarray}

Setting $\nabla_w E(\Vec{w}) \overset{!}{=} 0$ results in:
\begin{eqnarray}
\Mtx{\Phi}^T \Mtx{\Phi} \Vec{w} - \Mtx{\Phi}^T \Vec{z} &=& 0\\
\Mtx{\Phi}^T \Mtx{\Phi} \Vec{w} &=& \Mtx{\Phi}^T \Vec{z}\\
\Vec{w}_{ML} &=& (\Mtx{\Phi}^T \Mtx{\Phi})^{-1}\Mtx{\Phi}^T \Vec{z}
\end{eqnarray}


\item Generalize the normal equations to the case of arbitrary $\theta_n$.

\paragraph*{Solution:}
\begin{eqnarray}
\nabla_w E(\Vec{w}) &=& -\Mtx{\Phi}^T (\Mtx{\Theta} + \Mtx{\Theta}^T) (\Vec{z}-\Mtx{\Phi} \Vec{w}) = - \Mtx{\Phi}^T 2\Mtx{\Theta} (\Vec{z}-\Mtx{\Phi} \Vec{w})\\
&=& 2 \Mtx{\Phi}^T \Mtx{\Theta} \Mtx{\Phi} \Vec{w} - 2 \Mtx{\Phi}^T \Mtx{\Theta} \Vec{z}
\end{eqnarray}

Setting $\nabla_w E(\Vec{w}) \overset{!}{=} 0$ results in:
\begin{eqnarray}
2 \Mtx{\Phi}^T \Mtx{\Theta} \Mtx{\Phi} \Vec{w} - 2 \Mtx{\Phi}^T \Mtx{\Theta} \Vec{z} &=& 0\\
\Mtx{\Phi}^T \Mtx{\Theta} \Mtx{\Phi} \Vec{w} &=& \Mtx{\Phi}^T \Mtx{\Theta} \Vec{z}\\
\Vec{w}_{ML} &=& (\Mtx{\Phi}^T \Mtx{\Theta} \Mtx{\Phi})^{-1} \Mtx{\Phi}^T \Mtx{\Theta} \Vec{z}
\end{eqnarray}


\item Suppose we have a training set ${(\Vec{x}_n, z_n);\ n = 1, \dots ,N}$ 
of $N$ independent examples, but in which the $z_n$ were observed with differing 
variances. Specifically, suppose that
\[
p(z_n|\Vec{x}_n,\Vec{w}) = \mathcal{N}(z_n|\Vec{w}^{T}\Vec{\phi}(\Vec{x}_n),\sigma_n^2)
\]
where the $\sigma_n$ are fixed, known, constants. Show that finding the maximum 
likelihood estimate of $\Vec{w}$ reduces to solving a weighted linear regression problem. 
State clearly what the $\theta_n$ are in terms of the $\sigma_n$.

\paragraph*{Solution:}
\begin{eqnarray}
&&ln (\mathcal{N}(z_n|\Vec{w}^{T}\Vec{\phi}(\Vec{x}_n),\sigma_n^2))\\
 &=& ln \prod^{N}_{n=1} p(z_n|\Vec{x}_n,\Vec{w}) \\
&=& ln \prod^{N}_{n=1} \frac{1}{\sqrt{2\pi \sigma_n^2}} exp\left(-\frac{1}{2} \frac{(z_n - \Vec{w}^{T}\Vec{\phi}(\Vec{x}_n))^2}{\sigma_n^2}\right)\\
&=& -\frac{N}{2} ln(2\pi)-\frac{1}{2} \sum^{N}_{n=1} ln(\sigma_n^2) -\frac{1}{2} \sum^{N}_{n=1} \frac{(z_n - \Vec{w}^{T}\Vec{\phi}(\Vec{x}_n))^2}{\sigma_n^2}\\
&=& -\frac{N}{2} ln(2\pi)-\frac{1}{2} \sum^{N}_{n=1} ln(\sigma_n^2) -\frac{1}{2} \sum^{N}_{n=1} \frac{1}{\sigma_n^2}(z_n - \Vec{w}^{T}\Vec{\phi}(\Vec{x}_n))^2
\end{eqnarray}

Now we choose $\theta_n = \frac{1}{\sigma_n^2}$. Therefore, we get:
\begin{eqnarray}
-\frac{N}{2} ln(2\pi)-\frac{1}{2} \sum^{N}_{n=1} ln(\sigma_n^2) -\frac{1}{2} \sum^{N}_{n=1} \theta_n (z_n - \Vec{w}^{T}\Vec{\phi}(\Vec{x}_n))^2
\end{eqnarray}

To minimize this term we derive w.r.t. $\Vec{w}$ and calculate a minimum solution for $\Vec{w}$ by setting the derivative equals to zero.

\begin{eqnarray}
\nabla_{\Vec{w}} ln(p(z_n|\Vec{x}_n,\Vec{w})) \propto \sum_{n=1}^N \theta_n (z_n - \Vec{w}^T\phi(\Vec{x_n}))\phi(\Vec{x_n}) \overset{!}{=} 0
\end{eqnarray}

Therefore, we showed that the finding of a MLE for a linear regression with different variances reduces to simply solving a weighted linear regression problem.

\item With \emph{ordinary} linear regression it may be a good idea to \emph{rescale} the
\emph{columns} of the design matrix -- in particular when using nonlinear basis function
expansions (e.g. like polynomial expansion). Using the normal equations, prove that rescaling
the design matrix does not change the predicted values for some test dataset.
\end{enumerate}

\paragraph*{Solution:}





\section{Assignment: Basisfunctions }
\paragraph*{Problem 5}
$\;$ 

Show that the $\tanh$ function and the logistic sigmoid function are related
by 
\[
\tanh(x) = 2\sigma(2x) - 1
\]
Thus, show that a general linear combination of logistic sigmoid functions
of the form
\[
y(x, \Vec{w}) = w_0 + \sum_{j=1}^{M} w_j \sigma\left(\frac{x - \mu_j}{s}\right)
\]
is equivalent to a linear combination of $\tanh$ functions of the form
\[
y(x, \Vec{u}) = u_0 + \sum_{j=1}^{M} u_j \tanh\left(\frac{x - \mu_j}{2s}\right)
\]
and find expressions to relate the new parameters $\{u_0, \dots, u_M\}$ to 
the original parameters $\{w_0, \dots, w_M \}$.

\paragraph*{Solution:}

In order to relate the two functions $y(x,w) and y(x,u)$, we transform $y(x,u)$ as follows:

\begin{eqnarray}
y(x,u) &=& u_0 + \sum_{j=1}^M u_j tanh(\frac{x-\mu_j}{2s}) \\
&=& u_0 + \sum_{j=1}^M u_j (2 \sigma(2*\frac{x-\mu_j}{2s}) -1) \\
&=& u_0 + \sum_{j=1}^M u_j (2 \sigma(\frac{x-\mu_j}{s})-1) \\
&=& u_0  - \sum_{j=1}^M u_j + \sum_{j=1}^M u_j 2 \sigma(\frac{x-\mu_j}{s}) \\
\end{eqnarray}

Therefore, the two equations can be set equal by assuming 
\begin{eqnarray}
w_0 = u_0 - \sum_{j=1}^M u_j \\
w_j = 2 u_j \forall_{j>0}
\end{eqnarray}

\section{Assignment: Ridge regression }
\paragraph*{Problem 6}
$\;$ 

Show that the following holds:
The ridge regression estimates can be obtained by ordinary
least squares regression on an augmented dataset: Augment the
design matrix $\Mtx{\Phi}$ with $p$ additional rows  $\sqrt{\lambda}\Mtx{I}$
and augment $\Vec{z}$ with $p$ zeros.

\paragraph*{Solution:}




\paragraph*{Problem 7}
$\;$ 

Using singular value decomposition of the design matrix $\Mtx{\Phi} =
\Mtx{U}\Mtx{D}\Mtx{V}^T$ show that the output on the training set fitted with
the ridge regression solution $\hat{\Vec{w}}^{ridge}$ can be written as 
\[
\sum_j \left( \frac{d_j^2}{d_j^2 + \lambda} \Vec{u}_j \Vec{u}_j^T \right) \Vec{z} 
\]
where $\Vec{u}_j$ are the columns of $\Mtx{U}$, $d_j$ the
elements of $\Mtx{D}$ and $\lambda$ the cost factor of the $\ell2$
regularization. What is the interpretation of this formula?

\paragraph*{Solution:}




\section{Assignment: Multi-output linear regression }
\paragraph*{Problem 8}
$\;$ 

In class, we only considered functions of the form $f: \R^n \rightarrow \R$. What
about the general case of $f: \R^n \rightarrow \R^m$? For linear regression with
multiple outputs, write down the loglikelihood formulation and derive the MLE of
the parameters.

\paragraph*{Solution:}





\section{Assignment: Bayesian Linear Regression }
\paragraph*{Problem 9}
$\;$ 

$\star$ 
We have seen that, as the size of a data set increases, the uncertainty
associated with the posterior distribution over model parameters decreases (see 
tower equalities). Prove the following matrix identity
\[
(\Mtx{M} + \Vec{v}\Vec{v}^T)^{-1} = \Mtx{M}^{-1} - 
\frac{(\Mtx{M}^{-1}\Vec{v})(\Vec{v}^T\Mtx{M}^{-1})}{1 + \Vec{v}^T\Mtx{M}^{-1}\Vec{v}}
\]
and, using it, show that the uncertainty $\sigma_N^2(\Vec{x})$ associated
with the bayesian linear regression function given by eq. (26) on the slides satisfies
\begin{equation}
\sigma_{N+1}^2(\Vec{x}) \leq \sigma_N^2(\Vec{x})
\label{eq:varred}
\end{equation}

\paragraph*{Solution:}





\end{document}
