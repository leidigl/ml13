\documentclass{article}
\usepackage{ml1_homework_template}
\usepackage{amsmath}
\usepackage{amssymb}

% please submit the corresponding pdf by email to
% homework@class,brml.org, and write "homework sheet xx" in the 
% title.  No more, no less!  (Instead of xx, however,
% put the decimal number of the homework sheet.)

% Please update the following line, only change XX to the homework
% sheet number
\title{homework sheet 07}


\author{
\name{Andre Seitz}\\
\imat{03622870}\\
\email{andre.seitz@mytum.de}
\And
\name{Linda Leidig} \\
\imat{03608416}\\
\email{linda.leidig@tum.de}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.


\renewcommand{\Vec}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\Mtx}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}


\begin{document}
\maketitle

\section{Assignment: The Gaussian Kernel}
\paragraph*{Problem 1}
$\;$ 

To show: $K(x,y) = exp\left(-\frac{|\Vec{x}-\Vec{y}|^2}{2 \sigma^2}\right)$ is a kernel.

\begin{eqnarray}
K(x,y) &=& exp\left(-\frac{|\Vec{x}-\Vec{y}|^2}{2 \sigma^2}\right)\\
&=& exp\left(-\frac{\Vec{x}^T\Vec{x}-2\Vec{x}^T\Vec{y}+\Vec{y}^T\Vec{y}}{2 \sigma^2}\right)\\
&=& \underbrace{exp\left( -\frac{\Vec{x}^T\Vec{x}}{2 \sigma^2}\right) exp\left( -\frac{\Vec{y}^T\Vec{y}}{2 \sigma^2}\right)}_{\text{1. has to be a kernel}} \underbrace{exp\left( \frac{\Vec{x}^T\Vec{y}}{\sigma^2}\right)}_{\text{2. has to be a kernel}} \label{two_terms}
\end{eqnarray}
If both are kernels the entire term is a kernel (rule 3).

To show that 1. is a kernel we have a look on the linear kernel $K_3(\Vec{x}, \Vec{y}) = \Vec{x}^T\Vec{y}$. With $\phi(z) = exp\left( -\frac{\Vec{z}^T\Vec{z}}{2\sigma^2}\right)$ and rule 4 we have the following kernel:
\begin{eqnarray}
K_3(\phi(\Vec{x}), \phi(\Vec{y})) &=& \phi(\Vec{x})^T\phi(\Vec{y})\\
&=& \left(exp\left( -\frac{\Vec{x}^T\Vec{x}}{2\sigma^2}\right)\right)^T exp\left( -\frac{\Vec{y}^T\Vec{y}}{2\sigma^2}\right)\\
&=&exp\left( -\frac{\Vec{x}^T\Vec{x}}{2\sigma^2}\right) exp\left( -\frac{\Vec{y}^T\Vec{y}}{2\sigma^2}\right)
\end{eqnarray}
The last step is possible since it is a scalar. Therefore, the first part is a kernel.

To show that 2. is a kernel we have a look on the linear kernel $K_3$ again and additionally we use the Taylor expansion. We want to prove that $exp(K_1(\Vec{x}, \Vec{y}))$ is a kernel if $K_1$ is a kernel. With the Taylor expansion we know
\begin{eqnarray}
exp(K_1(\Vec{x}, \Vec{y})) = \sum_{i = 0}^{\infty} \frac{(K_1(\Vec{x}, \Vec{y}))^i}{i!}
\end{eqnarray}
With rule 3 we know that $(K_1(\Vec{x}, \Vec{y}))^i$ is a kernel. From rule 2 with $a = \frac{1}{i!}$ we know that $\frac{(K_1(\Vec{x}, \Vec{y}))^i}{i!}$ is also a kernel. Applying rule 1 we get that $\sum_{i = 0}^{\infty} \frac{(K_1(\Vec{x}, \Vec{y}))^i}{i!}$ is also a kernel which leads to the conclusion that $exp(K_1(\Vec{x}, \Vec{y}))$ is a kernel. Here $K_1(\Vec{x}, \Vec{y}) = \frac{\Vec{x}^T\Vec{y}}{\sigma^2}$. We know that $\Vec{x}^T\Vec{y}$ is a kernel. Applying rule 3 leads to the conclusion that $\frac{\Vec{x}^T\Vec{y}}{\sigma^2}$ is also a kernel. Therefore, the second term of equation \ref{two_terms} is also a kernel.

We could show that both terms of equation \ref{two_terms} are kernels. Therefore, the whole equation is a kernel.


\paragraph*{Problem 2}
$\;$ 

To do: Determine $\phi(\Vec{x})$ so that
\begin{eqnarray}
\phi(\Vec{x})^T\phi(\Vec{y}) = exp(-\frac{|\Vec{x}-\Vec{y}|^2}{2 \sigma^2})
\end{eqnarray}

With the Taylor expansion we get:
\begin{eqnarray}
&&exp(-\frac{|\Vec{x}-\Vec{y}|^2}{2 \sigma^2})\\
&=& exp\left( -\frac{\Vec{x}^T\Vec{x}}{2 \sigma^2}\right) exp\left( -\frac{\Vec{y}^T\Vec{y}}{2 \sigma^2}\right) \sum_{n=0}^{\infty}\frac{\left(\frac{\Vec{x}^T\Vec{y}}{\sigma^{2n}} \right)^n}{n!}\\
&=& exp\left( -\frac{\Vec{x}^T\Vec{x}}{2 \sigma^2}\right) exp\left( -\frac{\Vec{y}^T\Vec{y}}{2 \sigma^2}\right) \sum_{n=0}^{\infty}\frac{\left(\Vec{x}^T\Vec{y} \right)^n}{n!\sigma^{2n}}\\
&=& exp\left( -\frac{\Vec{x}^T\Vec{x}}{2 \sigma^2}\right) exp\left( -\frac{\Vec{y}^T\Vec{y}}{2 \sigma^2}\right) \sum_{n=0}^{\infty}\frac{\left(\sum_{t=1}^m x_t y_t \right)^n}{n!\sigma^{2n}}\\
&=& exp\left( -\frac{\Vec{x}^T\Vec{x}}{2 \sigma^2}\right) exp\left( -\frac{\Vec{y}^T\Vec{y}}{2 \sigma^2}\right) \sum_{n=0}^{\infty}\frac{\sum_{k_1+...+k_m = n} \left(\begin{array}{c}
n\\k_1,...,k_m
\end{array}\right) \prod_{t=1}^m(x_t y_t)^{k_t}}{n!\sigma^{2n}}\\
&=& exp\left( -\frac{\Vec{x}^T\Vec{x}}{2 \sigma^2}\right) exp\left( -\frac{\Vec{y}^T\Vec{y}}{2 \sigma^2}\right) \sum_{n=0}^{\infty}\frac{\sum_{k_1+...+k_m = n} \frac{n!}{k_1!...k_m!} \prod_{t=1}^m(x_t y_t)^{k_t}}{n!\sigma^{2n}}\\
&=& exp\left( -\frac{\Vec{x}^T\Vec{x}}{2 \sigma^2}\right) exp\left( -\frac{\Vec{y}^T\Vec{y}}{2 \sigma^2}\right) \sum_{n=0}^{\infty}\sum_{k_1+...+k_m = n} \frac{1}{k_1!...k_m!\sigma^{2n}} \prod_{t=1}^m x_t^{k_t} \prod_{t=1}^m y_t^{k_t}\\
&=& \sum_{n=0}^{\infty}\sum_{k_1+...+k_m = n} \left[exp\left( -\frac{\Vec{x}^T\Vec{x}}{2 \sigma^2}\right) \frac{\prod_{t=1}^m x_t^{k_t}}{k_1!...k_m!\sigma^{2n}}\right] \left[exp\left( -\frac{\Vec{y}^T\Vec{y}}{2 \sigma^2}\right) \frac{ \prod_{t=1}^m y_t^{k_t}}{k_1!...k_m!\sigma^{2n}}\right]
\end{eqnarray}
The inner product of the last equation is split into $\Vec{x}$ and $\Vec{y}$ and is therefore the inner product of the infinite-dimensional feature space.


\paragraph*{Problem 3}
$\;$ 

For linear regression with RBFs one has to train the weights corresponding to all the feature vectors, to fit the data  with these basis functions as good as possible. As one has to calculate the pseudo inverse of the feature vectors/basis functions, the complexity is cubic with respect to the number of basis functions. In higher dimensional feature spaces, this may lead to unbearable computational expensiveness, as the number of feature vectors increases exponentially with the number of dimensions. To handle this problem, the dual representation of the RBFs can be derived, so one has only to calculate the pseudo inverse of the kernel matrix. As the calculation of the kernel matrix only depends on the number of training samples, the complexity reduces to cubic with respect to the number of training samples.

Concrete, for linear regression with RBFs this results in the weighted sum of as many bumps as there are basis functions and for linear regression with the Gaussian kernel as many bumps as there are sample points are summed up weighted.


\section{Assignment: Kernel Perceptron}
\paragraph*{Problem 4}
$\;$ 

The perceptron learning rule from the lecture stated:
\begin{eqnarray}
w \leftarrow 
\begin{cases}
w + x_i $ if $ z_i  = 1 \\
w - x_i $ else$
\end{cases}
\end{eqnarray}
with
\begin{eqnarray}
z_i = sgn(\phi(x)^Tw +b)
\end{eqnarray}
Let i index the steps of updating the learning rule or better the subsets of the sample points, that were still misclassified during the update steps, $t_i \in {-1,1}$ and $\alpha_i$ being the number of times, the sample points $x_i$ where used during the whole update procedure. The perceptron learning rule can also be written as a linear combination:
\begin{eqnarray}
w = \sum_{i=1}^{l} \alpha_i t_i \phi(x_i)
\label{lineq}
\end{eqnarray}
and the prediction rule can be derived as 
\begin{eqnarray}
z &=& sgn(\phi(x^T) w+b)\\
&=& sgn(\phi(x)^T \sum_{i=1}^{l} \alpha_i t_i \phi(x_i) +b)\\
&=& sgn(\sum_{i=1}^{l} \alpha_i t_i (\phi(x)^T \phi(x_i)) +b)
\end{eqnarray}
As $alpha_i$ is incremented, whenever the subset indexed by $i$ is still misclassified during the update steps, one can write formally:
\begin{eqnarray}
\alpha_i \rightarrow \alpha_i +1\\
t_i (w^T \phi(x_i)) \geq 0 
\end{eqnarray}
Using \ref{lineq} and knowing $\alpha_n \geq 0$ we get:
\begin{eqnarray}
t_i \left( \left( \sum_{n=1}^{l} \alpha_n t_n \phi(x_n)\right)^T \phi(x_i) \right) = 
t_i \left( \left( \sum_{n=1}^{l} \phi(x_n)\right)^T \phi(x_i)\right) =
t_i\left(\sum_{n=1}^{l} K(x_n,x_i)\right) \geq 0 
\end{eqnarray}
This obviously shows, that the learning algorithm only depends on the kernel,and  that the feature vector $\phi(x)$ enters only in form of the kernel function $K(x,y) = \phi(x)^T\phi(y)$.


\section{Assignment: Kernelized k-nearest neighbors}
\paragraph*{Problem 5}
$\;$ 

\section{Assignment: Convex functions}
\paragraph*{Problem 6}
$\;$ 

Given:
\begin{itemize}
\item $f(\Vec{x})$ is convex $\Rightarrow$ $f(t\Vec{x} + (1-t)\Vec{y}) \leq tf(\Vec{x})+(1-t)f(\Vec{y})$
\item $g(\Vec{x})$ is convex $\Rightarrow$ $g(t\Vec{x} + (1-t)\Vec{y}) \leq tg(\Vec{x})+(1-t)g(\Vec{y})$
\end{itemize}
for $t \in [0,1]$ and any two points $\Vec{x}$, $\Vec{y}$.

1. To show: $h(\Vec{x}) = f(\Vec{x}) + g(\Vec{x})$ is convex.

\begin{eqnarray}
h(t\Vec{x} + (1-t)\Vec{y}) &=& f(t\Vec{x} + (1-t)\Vec{y}) + g(t\Vec{x} + (1-t)\Vec{y})\\
&\leq& [tf(\Vec{x})+(1-t)f(\Vec{y})] + [tg(\Vec{x})+(1-t)g(\Vec{y})]\\
&=& tf(\Vec{x}) + tg(\Vec{x}) + (1-t)f(\Vec{y}) + (1-t)g(\Vec{y})\\
&=& t(f(\Vec{x}) + g(\Vec{x})) + (1-t)(f(\Vec{y}) + g(\Vec{y}))\\
&=& th(\Vec{x}) + (1-t)h(\Vec{y})
\end{eqnarray}

2. To show: $u(\Vec{x}) = cf(\Vec{x})$ with $c \geq 0$ is convex.

\begin{eqnarray}
u(t\Vec{x} + (1-t)\Vec{y}) &=& cf(t\Vec{x} + (1-t)\Vec{y})\\
&\leq& c(tf(\Vec{x})+(1-t)f(\Vec{y}))\\
&=& tcf(\Vec{x})+(1-t)cf(\Vec{y})\\
&=& tu(\Vec{x})+(1-t)u(\Vec{y})
\end{eqnarray}


\paragraph*{Problem 7}
$\;$ 

\paragraph*{Problem 8}
$\;$ 

\end{document}
