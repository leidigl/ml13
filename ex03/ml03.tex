\documentclass{article}
\usepackage{ml1_homework_template}

% please submit the corresponding pdf by email to
% homework@class,brml.org, and write "homework sheet xx" in the 
% title.  No more, no less!  (Instead of xx, however,
% put the decimal number of the homework sheet.)

% Please update the following line, only change XX to the homework
% sheet number
\title{homework sheet 03}


\author{
\name{Andre Seitz}\\
\imat{03622870}\\
\email{andre.seitz@mytum.de}
\And
\name{Linda Leidig} \\
\imat{03608416}\\
\email{linda.leidig@tum.de}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\usepackage{MnSymbol}


\begin{document}
\maketitle

\section{Assignment: Basic Probability}
\paragraph*{Problem 1}
$\;$ 

$T \hateq $ terrorist and $\neg T \hateq $ not a terrorist\\
$S \hateq $ positive scan as terrorist and $\neg S \hateq $ negative scan as terrorist

The following information about probabilities are included in the text:

$p(T) = 0.01$, $p(\neg T) = 0.99$\\
$p(S|T) = 0.95$, $p(\neg S|T) = 0.05$\\
$p(\neg S|\neg T) = 0.95$, $p(S|\neg T) = 0.05$

To find: $p(T|S)$

\begin{eqnarray}
p(T|S)&=&\frac{p(S|T)p(T)}{p(S)}\\
&=&\frac{p(S|T)p(T)}{p(S|T)p(T)+p(S|\neg T)p(\neg T)}\\
&=&\frac{0.95\cdot 0.01}{0.95\cdot 0.01 + 0.05 \cdot 0.99}\\
&\approx & 0.161 
\end{eqnarray}


\paragraph*{Problem 2}
$\;$ 

$B \hateq $ Box and $D \hateq $ Drawn\\
$r \hateq $ red and $w \hateq $ white

To find: $p(B = 2red|D = 3 red)$

\begin{eqnarray}
p(B = 2r|D = 3r) &=& \frac{p(D = 3r |B = 2r)p(B = 2r)}{p(D = 3r)}\\
&=&\frac{p(D = 3r |B = 2r)p(B = 2r)}{p(D = 3r|B = 2r)p(B = 2r)+p(D = 3r|B = 2w)p(B = 2w)+ p(D = 3r|B = 1r1w)p(B = 1r1w)}\\
&=&\frac{1\cdot (0.5 \cdot 0.5)}{1\cdot (0.5 \cdot 0.5) + 0 \cdot (0.5 \cdot 0.5) + (0.5 \cdot 0.5 \cdot 0.5) \cdot (2 \cdot 0.5 \cdot 0.5)}\\
&=&\frac{0.25}{0.25 + 0.125 \cdot 0.5}\\
&=& \frac{0.25}{0.3125}\\
&=& 0.8
\end{eqnarray}


\paragraph*{Problem 3}
$\;$ 

Mean:
\begin{eqnarray}
E[X] &=& \int_{-\infty}^{\infty}x \cdot p(x) dx\\
&=& \int_0^1 x \cdot 1\\
&=& \left[ \frac{1}{2} x^2\right] _0^1\\
&=& \frac{1}{2} - 0\\
&=& \frac{1}{2}
\end{eqnarray}

Variance:
\begin{eqnarray}
Var[X] &=& E[X^2] - E[X]^2\\
&=& \int_{-\infty}^{\infty}x^2 \cdot p(x) dx - \left(\frac{1}{2}\right)^2\\
&=& \left[\frac{1}{3} x^3\right]_0^1 - \frac{1}{4}\\
&=& \frac{1}{3} - 0 - \frac{1}{4}\\
&=& \frac{1}{12}
\end{eqnarray}

\paragraph*{Problem 4}
$\;$ 

\textbf{Equation 1:}

\begin{eqnarray}
E[X] &=& \int_{-\infty}^{\infty}{x \cdot p(x)} dx \\
&=& \int_{-\infty}^{\infty}{x \int_{-\infty}^{\infty}{p_{X,Y}(x,y)} dy} dx\\
&=& \int_{-\infty}^{\infty}{x \int_{-\infty}^{\infty}{p_Y(y)\cdot p_{X|Y}(x|y)} dy} dx\\
&=& \int_{-\infty}^{\infty}{p_Y(y) \int_{-\infty}^{\infty}{x \cdot p_{X|Y}(x|y)} dx} dy\\
&=& \int_{-\infty}^{\infty}{p_Y(y) E_{X|Y}[X]} dy\\
&=& E_Y[E_{X|Y}[X]]
\end{eqnarray}

\textbf{Equation 2:}

With usage of equation1: 
\begin{eqnarray}
E_Y[Var_{X|Y}(X)] + Var_Y[E_{X|Y}(X)] &=& \\
E_Y[E_{X|Y}(X^2) - E_{X|Y}(X)^2] + E_Y[E_{X|Y}(X)^2] - E_Y[E_{X|Y}(X)]^2  &=& \\
E_Y[E_{X|Y}(X^2)] - E_Y[E_{X|Y}(X)^2] + E_Y[E_{X|Y}(X)^2] - E_Y[E_{X|Y}(X)]^2  &=& \\
E_Y[E_{X|Y}(X^2)] - E_Y[E_{X|Y}(X)]^2  &=& \\
E[X^2] - E[X]^2 &=&\\
Var[X]
\end{eqnarray}

\section{Assignment: Probability Inequalities}
\subsection{Markov Inequality}

\paragraph*{Problem 5}
$\;$ 

To show:
\begin{eqnarray}
P(X > c) \leq \frac{E[X]}{c}
\end{eqnarray}

\begin{eqnarray}
P(X > c) &\leq& \frac{E[X]}{c}\\
\sum_{x>c}{p(x)} &\leq& \frac{\sum_{x}{x \cdot p(x)}}{c}\\
\sum_{x>c}{p(x)} &\leq& \frac{\sum_{x\leq c}{x \cdot p(x)} + \sum_{x>c}{x \cdot p(x)}}{c}\\
0 &\leq& \frac{\sum_{x\leq c}{x \cdot p(x)} + \sum_{x>c}{x \cdot p(x)}}{c} - \sum_{x>c}{p(x)}\\
0 &\leq& \sum_{x\leq c}{x \cdot p(x)} + \sum_{x>c}{x \cdot p(x)} - \sum_{x>c}{ c \cdot p(x)} \label{3sums}
\end{eqnarray}

The first sum in the right term of inequality \ref{3sums} is greater 0 since the random variable is non-negative and probabilities are always greater than 0.

For the second and third sum the following holds:

\begin{eqnarray}
\sum_{x>c}{ c \cdot p(x)} &\leq& \sum_{x>c}{x \cdot p(x)}
\end{eqnarray}
since each element of the right sum is greater than the corresponding element of the left due to the relation $x > c$. Therefore,
\begin{eqnarray}
0 \leq \sum_{x>c}{x \cdot p(x)} - \sum_{x>c}{ c \cdot p(x)}
\end{eqnarray}
This proves that the relation in inequality \ref{3sums} is correct. Therefore, the Markov inequality is proved.

Application of the Markov inequality:

Let X be a random variable for drawing a head by flipping a fair coin n times.
\begin{eqnarray}
P(X > \frac{3}{4}n) \leq \frac{E[X]}{\frac{3}{4}n}
\end{eqnarray}
With $E[X] = \frac{1}{2}n$ we get:
\begin{eqnarray}
P(X > \frac{3}{4}n) \leq \frac{\frac{1}{2}n}{\frac{3}{4}n} = \frac{2}{3}
\end{eqnarray}


\subsection{Chebyshev Inequality}

\paragraph*{Problem 6}
$\;$ 

To show:
\begin{eqnarray}
P(|X - E[X]| > a) \leq \frac{Var[X]}{a^2}
\end{eqnarray}

From the Markov inequality we know:
\begin{eqnarray}
P(X > a) \leq \frac{E[X]}{a}
\end{eqnarray}

Computing $P(|X - E[X]| > a) = P((X - E[X])^2 > a^2)$ using the Markov inequality leads to the following:
\begin{eqnarray}
P(X > a) &\leq& \frac{E[X]}{a}\\
\Leftrightarrow P((X - E[X])^2 > a^2) &\leq& \frac{E[(X - E[X])^2]}{a^2}\\
\Leftrightarrow P(|X - E[X]| > a) &\leq& \frac{Var[X]}{a^2}
\end{eqnarray}


Application of the Cheybyshev inequality:

Let X be a random variable for drawing a head by flipping a fair coin n times.
\begin{eqnarray}
E[X] = \frac{1}{2}n\\
Var[X] = \frac{1}{4}n
\end{eqnarray}
With this we get:
\begin{eqnarray}
P(X > \frac{3}{4}n) = P\left(\left|X-\frac{1}{2}n\right| > \frac{1}{4}n\right) \leq \frac{\frac{1}{4}n}{\left(\frac{1}{4}n\right)^2} = \frac{4}{n}
\end{eqnarray}


\subsection{Jensen's Inequality}

\paragraph*{Problem 7}
$\;$ 

\textbf{I-Begin} ($n=2$): obvious per definition of convex functions\\
\textbf{I-Statement} ($n$): Assume
\begin{eqnarray}
f(\lambda_1 x_1+\dots + \lambda_n x_n) \leq \lambda f(x_1) + \dots +\lambda_n f(x_n)
\end{eqnarray}
\textbf{I-Step} ($n \leftarrow n+1$):
\begin{eqnarray}
f(\lambda_1 x_1 + \dots +\lambda_n x_n + \lambda_{n+1} x_{n+1}) &=& \\
f(\sum_{i=1}^{n+1} \lambda_i x_i) &=& \\
f(\lambda_1 x_1 + \sum_{i=2}^{n+1} \lambda_ix_i) &=& \\
f(\lambda_1 x_1 + (1-\lambda_1)\sum_{i=2}^{n+1} \frac{\lambda_i}{1-\lambda_1}x_i) &\leq & \\
\lambda_1 f(x_1) + (1-\lambda_1)f(\sum_{i=2}^{n+1} \frac{\lambda_i}{1-\lambda_1}x_i) &\leq & 
\sum_{i=1}^{n+1}\lambda_i f(x_i)
\end{eqnarray}




\end{document}
