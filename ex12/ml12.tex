\documentclass{article}
\usepackage{ml1_homework_template}
\usepackage{amsmath}
\usepackage{amssymb}
% please submit the corresponding pdf by email to
% homework@class,brml.org, and write "homework sheet xx" in the 
% title.  No more, no less!  (Instead of xx, however,
% put the decimal number of the homework sheet.)

% Please update the following line, only change XX to the homework
% sheet number
\title{homework sheet 12}


\author{
\name{Andre Seitz}\\
\imat{03622870}\\
\email{andre.seitz@mytum.de}
\And
\name{Linda Leidig} \\
\imat{03608416}\\
\email{linda.leidig@tum.de}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\renewcommand{\Vec}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\Mtx}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}

\usepackage{graphicx}

\begin{document}
\maketitle

\section{Assignment: K-Means and MoG}
\paragraph*{Problem 1}
$\;$ 

given: K isotropic Gaussians, each with covariance $\Mtx{\Sigma} = \sigma^2 \Mtx{I}$

To show: If $\sigma \rightarrow 0$, the EM algorithm for MoG converges to the K-means algorithm.

If the covariances converge to 0, the center of the Gaussians i.e. the location of the mean has a very high value while its surrounding converges to 0. Therefore, each Gaussian can be represented as a single point correlating with the center of the clusters of the K-means.

formal solution: In the E-step for MoG the posterior probability or responsibility $\gamma(z_k) = p(z_k = 1 | \Vec{x})$ has to be evaluated where $z_k$ is in the 1-of-K coding scheme. This comes up with the probability that the $k^{th}$ Gaussian is responsible for the data point $\Vec{x}$ under the condition that $\Vec{x}$ is already known.

\begin{eqnarray}
\gamma(z_k) &=& p(z_k=1|\Vec{x})\\
&=& \frac{p(z_k=1)p(\Vec{x}|z_k=1)}{\sum_{j=1}^{K}p(z_j=1)p(\Vec{x}|z_j=1)}\\
&=& \frac{\pi_k \mathcal{N}(\Vec{x}|\Vec{\mu_k}, \Mtx{\Sigma})}{\sum_{j=1}^K \pi_j \mathcal{N}(\Vec{x}|\Vec{\mu_j}, \Mtx{\Sigma})}\\
&=& \frac{\pi_k exp\left(\frac{-||\Vec{x}-\Vec{\mu_k}||^2}{2\sigma^2}\right)}{\sum_{j=1}^K \pi_j exp\left(\frac{-||\Vec{x}-\Vec{\mu_j}||^2}{2\sigma^2}\right)}\\
&=& \frac{1}{\frac{1}{\pi_k exp\left(\frac{-||\Vec{x}-\Vec{\mu_k}||^2}{2\sigma^2}\right)}\sum_{j=1}^K \pi_j exp\left(\frac{-||\Vec{x}-\Vec{\mu_j}||^2}{2\sigma^2}\right)}\\
&=& \frac{1}{\sum_{j=1}^K \frac{\pi_j}{\pi_k} exp\left(\frac{-||\Vec{x}-\Vec{\mu_j}||^2 + ||\Vec{x}-\Vec{\mu_k}||^2}{2\sigma^2}\right)}
\end{eqnarray}

Now we have to consider two cases:
\begin{enumerate}
\item $k$ is the closest component to $\Vec{x}$: The difference between the data point $\Vec{x}$ and the mean of $k$ is the smallest such that all the other distances between $\Vec{x}$ and the means of the other Gaussians is bigger. Therefore, the following holds $-||\Vec{x}-\Vec{\mu_j}||^2 + ||\Vec{x}-\Vec{\mu_k}||^2 \leq 0$ for all $j$. If $\sigma \rightarrow 0$, the value within the $exp$ is $-\infty$ for all $j \neq k$ and for $k$ it is equal to 0. Therefore, the entire denominator is 1 and with it the whole value for $\gamma(z_k)$.
\item $k$ is not the closest component to $\Vec{x}$: Then another $c$ exists that is the closest Gaussian to $\Vec{x}$. For this the following holds: $-||\Vec{x}-\Vec{\mu_c}||^2 + ||\Vec{x}-\Vec{\mu_k}||^2 > 0$. For $\sigma \rightarrow 0$, the value within the $exp$ converges to $+\infty$. Therefore, the whole denominator and whole value for $\gamma(z_k)$ converge to $+\infty$.
\end{enumerate}

All in all this is equal to the assignment step in K-means since we only consider to different values, namely 1 and $\infty$.

The M-step is equivalent for both algorithms since the new means are computed by optimization.
 

\paragraph*{Problem 2}
$\;$ 
Considering a mixture of Gaussians:
\begin{eqnarray}
p(x) = \sum_k \pi_k \mathcal{N}(x|\mu_k,\Sigma_k)
\end{eqnarray}
the expectation $E(x)$ and covariance $Cov(x)$ evaluates to:
\begin{eqnarray}
E(x) = E_Z(E_{X|Z}(x|z))
\end{eqnarray}
according to the so called tower rule (proven earlier during this course)
\begin{eqnarray}
E_Z(E_{X|Z}(x|z)) = \sum_k \pi_kE_{X|Z}(x|z)
\end{eqnarray}
per definition of the expectation and last but not least according to the definition of the mixture of gaussians:
\begin{eqnarray}
\sum_k \pi_kE_{X|Z}(x|z) = \sum_k \pi_k\mu_k
\end{eqnarray}
To use the identity
\begin{eqnarray}
Cov(x) = E(xx^T) - E(x)E(x)^T
\end{eqnarray}
one has to calculate
\begin{eqnarray}
E(xx^T) = E_Z(E_{X|Z}(xx^T|z)) = \sum_k \pi_k E(xx^T|z)
\end{eqnarray}
By using 
\begin{eqnarray}
E(xx^T|z) = \Sigma_k + \mu_k\mu_k^T
\end{eqnarray}
we finally get
\begin{eqnarray}
Cov(x) = \sum_k \pi_k (\Sigma_k + \mu_k\mu_k^T) - E(x)E(x)^T
\end{eqnarray}


\section{Assignment: FA/pPCA and PCA}
\paragraph*{Problem 3}
$\;$



\paragraph*{Problem 4}
$\;$ 

The posterior distribution is $p(z_i|x_i) = \mathcal{N}(z_i|m_i, \Mtx{\Sigma})$. Its variance and mean are
\begin{eqnarray}
\Mtx{\Sigma} = \sigma^2 (\Mtx{W}^T\Mtx{W}+\sigma^2\Mtx{I})^{-1}\\
m_i = (\Mtx{W}^T\Mtx{W} + \sigma^2\Mtx{I})^{-1} (\Mtx{W}^T (x_i-\mu))
\label{mean}
\end{eqnarray}
(see Bishop (12.42)).

Furthermore, we have to consider $\Mtx{W}$ since it is dependent on $\sigma^2$.
The maximum likelihood solution is given in Bishop (12.45)
\begin{eqnarray}
\Mtx{W}_{ML} = \Mtx{U}_M(\Mtx{L}_M-\sigma^2\Mtx{I})^{1/2}\Mtx{R}
\end{eqnarray}
With $\sigma^2 \rightarrow 0$ $\Mtx{W}$ and choosing $\Mtx{R}=\Mtx{I}$ it converges to
\begin{eqnarray}
\Mtx{W}_{ML} \rightarrow \Mtx{U}_M \Mtx{L}_M^{1/2}
\end{eqnarray}
Using this in the first part of equation \ref{mean} results in the following
\begin{eqnarray}
(\Mtx{W}^T\Mtx{W} + \sigma^2\Mtx{I})^{-1} \rightarrow ((\Mtx{U}_M \Mtx{L}_M^{1/2})^T(\Mtx{U}_M \Mtx{L}_M^{1/2}))^{-1} = ((\Mtx{L}_M^{1/2})^T \Mtx{U}_M^T \Mtx{U}_M \Mtx{L}_M^{1/2})^{-1} = \Mtx{L}_M^{-1}
\end{eqnarray}
Inserting this in \ref{mean}:
\begin{eqnarray}
m_i &=& \Mtx{L}_M^{-1} (\Mtx{U}_M \Mtx{L}_M^{1/2})^T (x_i - \mu)\\
&=& \Mtx{L}_M^{-1} (\Mtx{L}_M^{1/2})^T \Mtx{U}_M^T (x_i - \mu)\\
&=& \Mtx{L}_M^{-1/2} \Mtx{U}_M^T (x_i - \mu)
\end{eqnarray}
This is an orthogonal projection onto the same principal subspace as in PCA. (see Bishop (12.24))

\end{document}
